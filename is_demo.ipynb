{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of Data Generation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import adjusted_rand_score, silhouette_score, calinski_harabasz_score\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Function to generate donut-shaped data with an inner cluster\n",
    "def make_donut_data(num_classes=2, num_samples=100, radius_delta=1):\n",
    "    circle_all = np.array([])\n",
    "    class_lbls = np.array([])\n",
    "    \n",
    "    current_radius = radius_delta * num_classes\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        theta = inner_theta = np.random.uniform(0, 2*np.pi, num_samples)\n",
    "\n",
    "        # Donut data\n",
    "        circle_x = current_radius * np.cos(theta)\n",
    "        circle_y = current_radius * np.sin(theta)\n",
    "\n",
    "        circle_indiv = np.array([circle_x, circle_y]).T\n",
    "        circle_all = np.vstack((circle_all, circle_indiv)) if circle_all.size else circle_indiv\n",
    "\n",
    "        class_indiv = np.full(num_samples, i)\n",
    "        class_lbls = np.concatenate((class_lbls, class_indiv)) if class_lbls.size else class_indiv\n",
    "\n",
    "        current_radius = current_radius - radius_delta\n",
    "\n",
    "        \n",
    "    return circle_all, class_lbls\n",
    "\n",
    "def make_sinusoidal_wave(amplitude=1, frequency=1.5, phase=0, num_samples=100, num_classes=2, y_difference = 2):\n",
    "    # Generating x values\n",
    "    x = np.linspace(0, 4 * np.pi, num_samples)\n",
    "\n",
    "    x_all = np.array([])\n",
    "    y_all = np.array([])\n",
    "\n",
    "    class_lbls = np.array([])\n",
    "\n",
    "    diff = 0\n",
    "\n",
    "    # Calculating the sinusoidal wave\n",
    "    for i in range(num_classes):\n",
    "        x_all = np.concatenate((x_all, [x]), axis=1) if x_all.size else np.array([x])\n",
    "\n",
    "        y_indiv = amplitude * np.sin(frequency * x + phase) + diff\n",
    "        y_all = np.concatenate((y_all, [y_indiv]), axis=1) if y_all.size else np.array([y_indiv])\n",
    "        diff = diff + y_difference\n",
    "\n",
    "        class_indiv = np.full(num_samples, i)\n",
    "        class_lbls = np.concatenate((class_lbls, class_indiv)) if class_lbls.size else class_indiv\n",
    "\n",
    "    return np.vstack((x_all, y_all)).T, class_lbls\n",
    "\n",
    "def make_pinwheel_data(num_samples=100, num_classes=2, radius=1, stddev=0):\n",
    "    # Generating the data for the pinwheel\n",
    "    angle = np.linspace(0, 2 * np.pi, num_samples)\n",
    "    data = np.zeros((num_samples * num_classes, 2))\n",
    "    class_lbls = np.array([])\n",
    "    for i in range(num_classes):\n",
    "        ix = range(num_samples * i, num_samples * (i + 1))\n",
    "        r = np.linspace(0.0, radius, num_samples) + np.random.normal(0, stddev, num_samples)\n",
    "        data[ix] = np.c_[r * np.cos(angle + i * (2 * np.pi) / num_classes),\n",
    "                         r * np.sin(angle + i * (2 * np.pi) / num_classes)]\n",
    "\n",
    "        class_indiv = np.full(num_samples, i)\n",
    "        class_lbls = np.concatenate((class_lbls, class_indiv)) if class_lbls.size else class_indiv\n",
    "\n",
    "    return data, class_lbls\n",
    "\n",
    "def make_bar_data(num_samples=100, num_classes=2, x_init = 0.0, y = 0.5, distance_delta = 1):\n",
    "    # Generating points for bars\n",
    "    x = x_init\n",
    "    bar_all = y_all = np.array([])\n",
    "    class_lbls = np.array([])\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        bar_indiv = np.random.rand(num_samples, 2) * 0.5 + np.array([x, y])\n",
    "        bar_all = np.vstack((bar_all, bar_indiv)) if bar_all.size else bar_indiv\n",
    "        x = x + distance_delta\n",
    "\n",
    "        class_indiv = np.full(num_samples, i)\n",
    "        class_lbls = np.concatenate((class_lbls, class_indiv)) if class_lbls.size else class_indiv\n",
    "\n",
    "    return bar_all, class_lbls\n",
    "\n",
    "def get_card_data():\n",
    "    df = pd.read_csv('data/credit_card.csv')\n",
    "    # Dropping the CUST_ID column from the data\n",
    "    df = df.drop('CUST_ID', axis = 1)\n",
    "    # Handling the missing values if any\n",
    "    df.fillna(method ='ffill', inplace = True)\n",
    "\n",
    "    # Preprocessing the data to make it visualizable\n",
    " \n",
    "    # Scaling the Data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(df)\n",
    "    \n",
    "    # Normalizing the Data\n",
    "    X_normalized = normalize(X_scaled)\n",
    "    \n",
    "    # Converting the numpy array into a pandas DataFrame\n",
    "    X_normalized = pd.DataFrame(X_normalized)\n",
    "    \n",
    "    # Reducing the dimensions of the data\n",
    "    pca = PCA(n_components = 2)\n",
    "    X_principal = pca.fit_transform(X_normalized)\n",
    "    X_principal = pd.DataFrame(X_principal)\n",
    "    X_principal.columns = ['P1', 'P2']\n",
    "    \n",
    "    \n",
    "    return np.array(X_principal), None\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the colors to be used in the plots\n",
    "colors = ['red', 'orange', 'lime', 'green', 'blue', 'navy', 'violet']\n",
    "fig_size = 6\n",
    "\n",
    "# Params of Data Generation\n",
    "samples = 500  # Number of samples per class\n",
    "classes = 2     # Number of classes\n",
    "\n",
    "# X, Y = make_donut_data(num_classes=classes, num_samples=samples)\n",
    "# X, Y = make_sinusoidal_wave(num_samples=samples, num_classes=classes)\n",
    "# X, Y = make_pinwheel_data(num_samples=samples, num_classes=classes, stddev=0)\n",
    "# X, Y = make_bar_data(num_samples=samples, num_classes=classes)\n",
    "X, Y = get_card_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the Data Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8,8))\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], marker='.')\n",
    "plt.xlabel('x0')\n",
    "plt.ylabel('x1')\n",
    "plt.title('Generated Data')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics for Examining Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusted Rand Index (ARI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Adjusted Rand Index (ARI) is a measure used to assess the similarity between two data clusterings. It is an adjustment of the Rand Index and takes into account the expected similarity between random clusterings. The Rand Index measures the proportion of pairs of data points that are either in the same cluster or in different clusters in both the true and predicted clusterings.\n",
    "\n",
    "The Adjusted Rand Index adjusts the Rand Index for chance, providing a score that ranges from -1 to 1, where:\n",
    "\n",
    "- ARI = 1: Perfect similarity between the true and predicted clusterings.\n",
    "- ARI = 0: Random clustering (similarity is equivalent to chance).\n",
    "- ARI = -1: Perfect dissimilarity between the true and predicted clusterings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhoutte Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The silhouette score is a metric used to calculate the goodness of a clustering technique. It measures how well-defined the clusters are in a given set of data. The silhouette score ranges from -1 to 1, with a high value indicating that the object is well matched to its own cluster and poorly matched to neighboring clusters. If most objects have a high value, then the clustering configuration is appropriate. If many points have a low or negative value, then the clustering configuration may have too many or too few clusters.\n",
    "\n",
    "The Silhouette Score, provides a score that ranges from -1 to 1, where:\n",
    "- s(i) ≈ 1: The object is well matched to its own cluster and poorly matched to neighboring clusters. This is a good indication of a well-defined cluster.\n",
    "- s(i) ≈ 0: The object could be on the border of two clusters, or the data may be in a region where it is difficult to assign it to one cluster over another.\n",
    "- s(i) ≈ -1: The object is poorly matched to its own cluster and well matched to a neighboring cluster. This could indicate that the data point should be in a different cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Clustering Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition\n",
    "- K-means clustering is a widely used unsupervised machine learning algorithm used to partition a dataset into K distinct, non-overlapping clusters. \n",
    "- The primary objective of K-means is to group similar data points together based on their features. \n",
    "- Each cluster is represented by a centroid, which is the mean (average) of all data points belonging to that cluster.\n",
    "\n",
    "#### Advantages\n",
    "- Ease of Implementation\n",
    "- Efficiency\n",
    "- Fast Convergence\n",
    "- Interpretability\n",
    "- Versatility\n",
    "\n",
    "#### Disadvantages\n",
    "- Sensitivity to Initialization\n",
    "- Assumption of Equal-Sized, Spherical Clusters\n",
    "- Impact of Outliers\n",
    "- Non-Convex Clusters\n",
    "- Initialization Challenges\n",
    "- Cluster Shape Assumptions\n",
    "\n",
    "#### Use Cases\n",
    "- **Numeric Data**: K-means clustering works well with numerical data. It assumes that the features in the dataset are continuous and can be represented as Euclidean distances.\n",
    "- **Homogeneous Data**: It performs best with data where clusters are spherical, isotropic, and have similar densities.\n",
    "- **Linear Separability**: Effective for data with clear boundaries between clusters, assuming clusters are relatively well separated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Clustring Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "init_array = ['k-means++', 'random']\n",
    "algo_array = ['lloyd', 'elkan']\n",
    "\n",
    "fig, axs = plt.subplots(len(init_array), len(algo_array), figsize=(fig_size * len(init_array), fig_size * len(algo_array)))\n",
    "\n",
    "for i, init in enumerate(init_array):\n",
    "    for j, algo in enumerate(algo_array):\n",
    "        ax = axs[i, j]\n",
    "\n",
    "        model = KMeans(n_clusters=classes, init=init, algorithm=algo, n_init='auto').fit(X)\n",
    "        targ = model.predict(X)\n",
    "\n",
    "        for idx, cl in enumerate(np.unique(targ)):\n",
    "            ax.scatter(X[targ == cl, 0], X[targ == cl, 1], color=colors[idx], marker='.')\n",
    "\n",
    "        centroids = model.cluster_centers_\n",
    "        ax.scatter(centroids[:, 0], centroids[:, 1], marker='x', s=200, c='black', label='Centroids')\n",
    "        ax.legend()\n",
    "        ax.set_title(f\"init={init} algorithm={algo}\")\n",
    "\n",
    "        if Y is not None:\n",
    "            print(f\"Adjusted Rand Score        = {adjusted_rand_score(targ, Y):.2f} - KMeans CLustering - init={init} algorithm={algo}\")\n",
    "        print(f\"Silhouette Score           = {silhouette_score(X, targ):.2f} - KMeans CLustering - init={init} algorithm={algo}\\n\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuzzy C-Means Clustering Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition\n",
    "- Fuzzy C-Means Clustering is a type of unsupervised machine learning technique used for clustering data points into groups or clusters \n",
    "- It allows data points to belong to multiple clusters with varying degrees of membership.\n",
    "- Each data point is associated with a membership value for each cluster, indicating the degree to which it belongs to that cluster.\n",
    "\n",
    "#### Advantages\n",
    "- Soft Assignments\n",
    "- Robust to Noise\n",
    "- Flexibility\n",
    "- Granular Insights\n",
    "- Tolerant of Overlapping Clusters\n",
    "\n",
    "#### Disadvantages\n",
    "- Sensitivity to Initial Conditions\n",
    "- Computationally Intensive\n",
    "- Subjective Choice of Fuzziness Parameter\n",
    "- Lack of Convex Clusters\n",
    "- No Guaranteed Global Optimum\n",
    "\n",
    "#### Use Cases\n",
    "- **Fuzzy or Uncertain Data**: FCM is designed for data that is ambiguous or uncertain. It's suitable for situations where a data point might belong to multiple clusters simultaneously.\n",
    "- **Data with Partial Memberships**: When the underlying structure of the data suggests that some data points may belong to more than one cluster with varying degrees of membership.\n",
    "- **Mixed or Overlapping Clusters**: It's effective for datasets where clusters may have some degree of overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuzzy C-Means Clustering Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skfuzzy import cmeans, cmeans_predict\n",
    "\n",
    "centers, u, u0, d, jm, p, fpc = cmeans(data = X.T, c = classes, m = 1.25, error = 0.05, maxiter = 1000)\n",
    "target2 = np.argmax(u, axis=0)\n",
    "\n",
    "for idx, cl in enumerate(np.unique(target2)):\n",
    "    plt.scatter(X[target2 == cl, 0], X[target2 == cl, 1], color=colors[idx], marker='.')\n",
    "    \n",
    "plt.scatter(centers[:, 0], centers[:, 1], marker='x', s=200, c='black', label='Centroids')\n",
    "plt.legend()\n",
    "plt.title(\"Fuzzy C-Means Clustering\")\n",
    "\n",
    "# Printing membership matrix\n",
    "print(pd.DataFrame(u.T))\n",
    "\n",
    "if Y is not None:\n",
    "    print(f\"Adjusted Rand Score        = {adjusted_rand_score(target2, Y):.2f} - Fuzzy C-Means\")\n",
    "print(f\"Silhouette Score           = {silhouette_score(X, target2):.2f} - Fuzzy C-Means\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Clustering Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition\n",
    "- Spectral clustering is a clustering technique used in machine learning and data analysis to partition a dataset into cohesive clusters based on the spectral properties of a similarity or affinity matrix derived from the data. \n",
    "- It is a powerful clustering method that can uncover complex cluster structures, including non-convex and disconnected clusters. \n",
    "- Spectral clustering is particularly useful when traditional methods like K-means struggle to capture the underlying cluster relationships.\n",
    "\n",
    "#### Advantages\n",
    "- Ability to Capture Complex Structures\n",
    "- No Assumption of Cluster Shape\n",
    "- Robust to Noise and Outliers\n",
    "- Dimensionality Reduction\n",
    "- Cluster Visualization\n",
    "\n",
    "#### Disadvantages\n",
    "- Computational Complexity\n",
    "- Scaling Issues\n",
    "- Sensitivity to Affinity Measure\n",
    "- Lack of Deterministic Result\n",
    "- Interpretability of Eigenvectors\n",
    "\n",
    "#### Use Cases\n",
    "- **Non-linear Data**: Spectral clustering is particularly useful for non-linearly separable data. It's effective for identifying clusters with complex shapes and structures.\n",
    "- **Graph-Based Data**: Works well with data that can be represented as a similarity or distance graph. It can be applied to various data types, including numeric and non-numeric data.\n",
    "- **Manifold Structure**: Suitable for data with a manifold structure where traditional distance-based clustering methods might fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Clustering Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "aff_array = ['nearest_neighbors', 'rbf', 'poly', \n",
    "                'polynomial', 'laplacian', 'sigmoid']\n",
    "# you can use these kernels for affinity but they dont always work\n",
    "# ['additive_chi2', 'chi2', 'linear', 'cosine']\n",
    "\n",
    "fig, axs = plt.subplots(3, 2, figsize=(8, 12))\n",
    "\n",
    "for i, aff in enumerate(aff_array):\n",
    "    m = int(i / (len(aff_array) / 2))\n",
    "    n = int(i % (len(aff_array) / 2))\n",
    "    ax = axs[n, m]\n",
    "\n",
    "    targ = SpectralClustering(n_clusters=classes, affinity=aff, gamma=0.5).fit_predict(X)\n",
    "\n",
    "    for idx, cl in enumerate(np.unique(targ)):\n",
    "        ax.scatter(X[targ == cl, 0], X[targ == cl, 1], color=colors[idx], marker='.')\n",
    "        ax.set_title(f\"affinity={aff}\")\n",
    "\n",
    "    if Y is not None:\n",
    "        print(f\"Adjusted Rand Score        = {adjusted_rand_score(targ, Y):.2f} - Spectral Clustering - {aff}\")\n",
    "    print(f\"Silhouette Score           = {silhouette_score(X, targ):.2f} - Spectral Clustering - {aff}\\n\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomerative Clustering Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition\n",
    "- Agglomerative clustering is a hierarchical clustering technique used in machine learning and data analysis. \n",
    "- It is a bottom-up approach to clustering, where each data point initially forms its own cluster, and then these clusters are successively merged into larger clusters based on a similarity or dissimilarity metric. \n",
    "- The process continues until all data points belong to a single, all-encompassing cluster.\n",
    "- Recursively merges pair of clusters of sample data; uses linkage distance.\n",
    "\n",
    "#### Advantages\n",
    "- Simple to implement\n",
    "- Hierarchical structure\n",
    "- No fixed number of clusters required\n",
    "- Robust to noise\n",
    "\n",
    "#### Disadvantages\n",
    "- Computationally expensive\n",
    "- Sensitive to initial conditions\n",
    "- Difficulty with large datasets\n",
    "- Difficulty in handling non-globular shapes\n",
    "- Memory consumption\n",
    "\n",
    "#### Use Cases\n",
    "- **All Data Types**: Agglomerative clustering can handle different types of data, be it numerical or categorical, and can work with various distance/similarity measures.\n",
    "- **Large Datasets**: It can handle larger datasets, but computational complexity increases with the number of data points.\n",
    "- **Hierarchical Structure**: Effective for data with hierarchical structures, where the formation of clusters follows a nested or hierarchical pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomerative Clustering Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)\n",
    "\n",
    "def plot_data(model, lin, met, ax):\n",
    "    model.set_params(n_clusters=classes, distance_threshold=None)\n",
    "    targ = model.fit_predict(X)\n",
    "    \n",
    "    if Y is not None: \n",
    "        print(f\"Adjusted Rand Score        = {adjusted_rand_score(targ, Y):.2f} - Agglomerative Clustering - linkage={lin} metric={met}\")\n",
    "    print(f\"Silhouette Score           = {silhouette_score(X, targ):.2f} - Agglomerative Clustering - linkage={lin} metric={met}\\n\")\n",
    "\n",
    "    for idx, cl in enumerate(np.unique(targ)):\n",
    "        ax.scatter(X[targ == cl, 0], X[targ == cl, 1], color=colors[idx], marker='.')\n",
    "        ax.set_title(f\"linkage={lin} metric={met} Plot\")\n",
    "        \n",
    "\n",
    "met_array = ['cityblock', 'l1', 'l2', 'manhattan', 'euclidean']\n",
    "lin_array = ['complete', 'average', 'single'] # 'ward' only works on euclidian thus it is appended only in the end\n",
    "\n",
    "fig, axs = plt.subplots(len(lin_array) * len(met_array) + 1, 2, figsize=(fig_size * 2, fig_size * len(lin_array) * len(met_array) + 1))\n",
    "\n",
    "m = 0\n",
    "for i, met in enumerate(met_array):\n",
    "    if(met == 'euclidean'):\n",
    "        lin_array.append('ward')\n",
    "    for j, lin in enumerate(lin_array):\n",
    "        model = AgglomerativeClustering(distance_threshold=0, n_clusters=None, linkage=lin, metric=met)\n",
    "        model = model.fit(X)\n",
    "\n",
    "        plot_dendrogram(model, truncate_mode=\"level\", p=3, ax=axs[m, 0])\n",
    "        plot_data(model, lin, met, ax=axs[m, 1])\n",
    "        m = m + 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- **KMeans Clustering**: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "- **Fuzzy CMeans Clustering**: https://pythonhosted.org/scikit-fuzzy/api/skfuzzy.html#skfuzzy.cmeans\n",
    "- **Spectral Clustering**: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html\n",
    "- **Agglomerative Clustering**: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
